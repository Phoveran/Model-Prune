#!/bin/bash --login
########### SBATCH Lines for Resource Request ##########
#SBATCH --time=02:00:00      # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --exclude=lac-143
#SBATCH --nodes=1         # number of different nodes - could be an exact number or a range of nodes (same as -N)
#SBATCH --ntasks=1         # number of tasks - how many tasks (nodes) that you require (same as -n)
#SBATCH --cpus-per-task=3      # number of CPUs (or cores) per task (same as -c)
#SBATCH --mem-per-cpu=4G      # memory required per allocated CPU (or core) - amount of memory (in bytes)
#SBATCH --job-name exp  # you can give your job a name for easier identification (same as -J)
#SBATCH --gres=gpu:v100:1
#SBATCH --output=/dev/null   # modify it to the name you want for output
########## Command Lines to Run ##########
module purge
module load GCC/6.4.0-2.28 OpenMPI/2.1.2
module load CUDA/11.0.2 cuDNN/8.0.4.30-CUDA-11.0.2
export PATH=$PATH:$HOME/miniconda3/bin
source activate base
cd ~/workspace/projects/Model-Prune
python ${args}
scontrol show job $SLURM_JOB_ID   ### write job information to output file